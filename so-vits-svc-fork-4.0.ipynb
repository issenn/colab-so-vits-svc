{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before training\n",
    "\n",
    "This program saves the last 3 generations of models to Google Drive. Since 1 generation of models is >1GB, you should have at least 3GB of free space in Google Drive. If you do not have such free space, it is recommended to create another Google Account.\n",
    "\n",
    "Training requires >10GB VRAM. (T4 should be enough) Inference does not require such a lot of VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Connect to colab runtime and check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies\n",
    "#@markdown pip may fail to resolve dependencies and raise ERROR, but it can be ignored.\n",
    "!python -m pip install -U pip setuptools wheel\n",
    "%pip install -U ipython~=7.34.0\n",
    "\n",
    "#@markdown Branch (for development)\n",
    "BRANCH = \"none\" #@param {\"type\": \"string\"}\n",
    "if BRANCH == \"none\":\n",
    "    %pip install -U so-vits-svc-fork\n",
    "else:\n",
    "    %pip install -U git+https://github.com/34j/so-vits-svc-fork.git@{BRANCH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install rclone for OneDrive\n",
    "# !apt install -y fuse3\n",
    "!sudo -v ; curl https://rclone.org/install.sh | sudo bash\n",
    "!rclone -V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@markdown ### After the execution is completed, the runtime will **automatically restart**\n",
    "# exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Restart runtime**\n",
    "\n",
    "After running the cell above, you'll need to restart the Colab runtime because we installed a different version of numpy.\n",
    "\n",
    "`Runtime -> Restart runtime`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RCLONE_CONFIG = \"drive/MyDrive/rclone.conf\" #@param {type: \"string\"}\n",
    "CHARACTER = \"kiritan\" #@param {type: \"string\"}\n",
    "DO_EXTRACT_VOCALS = False\n",
    "CONFIG_FILE = \"configs/44k/config.json\" #@param {type: \"string\"}\n",
    "MODEL_PATH = \"drive/MyDrive/so-vits-svc-fork/logs/44k/\" #@param {type: \"string\"}\n",
    "MODEL_REPO_ID = \"issenn/kiritan\"\n",
    "KEEP_CKPTS = 10 #@param {type: \"number\"}\n",
    "CKPT_NAME_BY_STEP = True #@param {type: \"boolean\"}\n",
    "CONFIG_TYPE = \"so-vits-svc-4.0v1\" #@param [\"quickvc\", \"so-vits-svc-4.0v1-legacy\", \"so-vits-svc-4.0v1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount OneDrive\n",
    "# ![ ! -d onedrive ] && mkdir onedrive\n",
    "# !rclone mount colab:/ /content/onedrive --config {RCLONE_CONFIG} --vfs-cache-mode full --daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here, we override config to have num_workers=0 because\n",
    "of a limitation in HF Spaces Docker /dev/shm.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "\n",
    "def update_config(config_file=CONFIG_FILE):\n",
    "    config_path = Path(config_file)\n",
    "    data = json.loads(config_path.read_text())\n",
    "    # data['train']['eval_interval'] = 200\n",
    "    # data['train']['epochs'] = 10000\n",
    "    data['train']['learning_rate'] = 0.00005\n",
    "    # data['train']['batch_size'] = 32\n",
    "    data['train']['keep_ckpts'] = KEEP_CKPTS\n",
    "    data['train']['num_workers'] = multiprocessing.cpu_count()\n",
    "    data['train']['ckpt_name_by_step'] = CKPT_NAME_BY_STEP\n",
    "    # data['train']['persistent_workers'] = True\n",
    "    # data['train']['push_to_hub'] = True\n",
    "    # data['train']['repo_id'] = MODEL_REPO_ID # tuple(data['spk'])[0]\n",
    "    # data['train']['private'] = True\n",
    "    config_path.write_text(json.dumps(data, indent=2, sort_keys=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run all Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Clean dataset directory\n",
    "!rm -r \"dataset_raw_raw\"\n",
    "!rm -r \"dataset_raw\"\n",
    "!rm -r \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Make dataset directory\n",
    "!mkdir -p \"dataset_raw_raw\"\n",
    "!mkdir -p \"dataset_raw\"\n",
    "!mkdir -p \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Copy your dataset_raw_raw\n",
    "#@markdown **We assume that your dataset_raw_raw is in your Google Drive's `so-vits-svc-fork/dataset_raw_raw/(speaker_name)` directory.**\n",
    "DATASET_NAME = CHARACTER\n",
    "!cp -R /content/drive/MyDrive/so-vits-svc-fork/dataset_raw_raw/{DATASET_NAME}/ -t \"dataset_raw_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Automatic split audio files into multiple files\n",
    "!svc pre-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Copy your dataset_raw\n",
    "#@markdown **We assume that your dataset_raw is in your Google Drive's `so-vits-svc-fork/dataset_raw/(speaker_name)` directory.**\n",
    "DATASET_NAME = CHARACTER\n",
    "!cp -R /content/drive/MyDrive/so-vits-svc-fork/dataset_raw/{DATASET_NAME}/ -t \"dataset_raw/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download dataset (Tsukuyomi-chan JVS)\n",
    "#@markdown You can download this dataset if you don't have your own dataset.\n",
    "#@markdown Make sure you agree to the license when using this dataset.\n",
    "#@markdown https://tyc.rei-yumesaki.net/material/corpus/#toc6\n",
    "# !wget https://tyc.rei-yumesaki.net/files/sozai-tyc-corpus1.zip\n",
    "# !unzip sozai-tyc-corpus1.zip\n",
    "# !mv \"/content/つくよみちゃんコーパス Vol.1 声優統計コーパス（JVSコーパス準拠）/おまけ：WAV（+12dB増幅＆高音域削減）/WAV（+12dB増幅＆高音域削減）\" \"dataset_raw/tsukuyomi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Automatic preprocessing Resample to 44100Hz and mono\n",
    "!svc pre-resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Divide filelists and generate config.json\n",
    "!svc pre-config -t {CONFIG_TYPE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Backup configs file\n",
    "update_config()\n",
    "!cp -r configs drive/MyDrive/so-vits-svc-fork/\n",
    "!cp -r filelists drive/MyDrive/so-vits-svc-fork/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Download configs file\n",
    "!cp -r drive/MyDrive/so-vits-svc-fork/configs .\n",
    "!cp -r drive/MyDrive/so-vits-svc-fork/filelists ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Generate hubert and f0\n",
    "F0_METHOD = \"crepe\" #@param [\"crepe\", \"crepe-tiny\", \"parselmouth\", \"dio\", \"harvest\"]\n",
    "FORCE_REBUILD_ON = False #@param {type: \"boolean\"}\n",
    "if FORCE_REBUILD_ON:\n",
    "    !svc pre-hubert -fm {F0_METHOD}\n",
    "else:\n",
    "    !svc pre-hubert -fm {F0_METHOD} -nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Backup or download dataset hubert and f0\n",
    "DATASET_NAME = CHARACTER\n",
    "\n",
    "BACKUP_ON = True #@param {type: \"boolean\"}\n",
    "if BACKUP_ON:\n",
    "    !zip -r dataset.zip dataset\n",
    "    # !zip -r dataset.wav.zip dataset -i dataset/**/*.wav\n",
    "    # !zip -r dataset.data.pt.zip dataset -i dataset/**/*.data.pt\n",
    "\n",
    "    !mkdir -p drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/\n",
    "    !rclone mkdir colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG}\n",
    "\n",
    "    !rm -f *.{{md5,sha1,sha256}}.sum\n",
    "    !rclone hashsum MD5 . --output-file checksum.md5.sum --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone hashsum SHA1 . --output-file checksum.sha1.sum --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone hashsum SHA256 . --output-file checksum.sha256.sum --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    \n",
    "    # !cp -f dataset{{,.wav,.data.pt}}.zip drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/\n",
    "    !cp -f *.{{md5,sha1,sha256}}.sum drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/\n",
    "    !rclone copy . colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --no-traverse --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone copy . colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --no-traverse --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "    # !rclone check . drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/ --one-way --download --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone check . drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/ --one-way --download --differ - --missing-on-dst - --error - --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    # !rclone check . colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --one-way --download --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone check checksum.md5.sum colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --checkfile MD5 --one-way --download --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    !rclone check . colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --one-way --download --differ - --missing-on-dst - --error - --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "    # !rclone checksum MD5 checksum.md5.sum colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ --config {RCLONE_CONFIG} --one-way --download --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "    # !ps -ef | grep rclone\n",
    "    # !iftop -t -s 1 -n\n",
    "else:\n",
    "    !cp -f drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/*.{{md5,sha1,sha256}}.sum .\n",
    "    !rclone copy colab:/so-vits-svc-fork/datasets/{DATASET_NAME}/ . --config {RCLONE_CONFIG} --no-traverse --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "    !rclone check checksum.md5.sum . --checkfile MD5 --one-way --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "    # !rclone checksum MD5 checksum.md5.sum . --one-way --differ - --missing-on-dst - --error - --filter \"+ dataset{{,.wav,.data.pt}}.zip\" --filter \"- dataset/**\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "    # !unzip -d ./ drive/MyDrive/so-vits-svc-fork/datasets/{DATASET_NAME}/dataset.zip\n",
    "    !unzip -d ./ dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Check dataset\n",
    "!ls dataset/*/*/* | wc -l\n",
    "!ls dataset/*/*/*.wav | wc -l\n",
    "!ls dataset/*/*/*.data.pt | wc -l\n",
    "!ls -lt dataset/**/* | tail -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from logging import getLogger\n",
    "from pathlib import Path\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.strategies.ddp import DDPStrategy\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "import so_vits_svc_fork.utils\n",
    "\n",
    "from so_vits_svc_fork import utils\n",
    "from so_vits_svc_fork.logger import is_notebook\n",
    "\n",
    "from so_vits_svc_fork.train import VitsLightning, VCDataModule\n",
    "\n",
    "LOG = getLogger(__name__)\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "\n",
    "\n",
    "def train(\n",
    "    config_path: Path | str, model_path: Path | str, reset_optimizer: bool = False\n",
    "):\n",
    "    config_path = Path(config_path)\n",
    "    model_path = Path(model_path)\n",
    "\n",
    "    hparams = utils.get_backup_hparams(config_path, model_path)\n",
    "    utils.ensure_pretrained_model(model_path, hparams.model.get(\"type_\", \"hifi-gan\"))\n",
    "\n",
    "    datamodule = VCDataModule(hparams)\n",
    "    strategy = (\n",
    "        (\n",
    "            \"ddp_find_unused_parameters_true\"\n",
    "            if os.name != \"nt\"\n",
    "            else DDPStrategy(find_unused_parameters=True, process_group_backend=\"gloo\")\n",
    "        )\n",
    "        if torch.cuda.device_count() > 1\n",
    "        else \"auto\"\n",
    "    )\n",
    "    LOG.info(f\"Using strategy: {strategy}\")\n",
    "    trainer = pl.Trainer(\n",
    "        logger=TensorBoardLogger(\n",
    "            model_path, \"lightning_logs\", hparams.train.get(\"log_version\", 0)\n",
    "        ),\n",
    "        # profiler=\"simple\",\n",
    "        val_check_interval=hparams.train.eval_interval,\n",
    "        max_epochs=hparams.train.epochs,\n",
    "        check_val_every_n_epoch=None,\n",
    "        precision=\"16-mixed\"\n",
    "        if hparams.train.fp16_run\n",
    "        else \"bf16-mixed\"\n",
    "        if hparams.train.get(\"bf16_run\", False)\n",
    "        else 32,\n",
    "        strategy=strategy,\n",
    "        callbacks=[pl.callbacks.RichProgressBar()] if not is_notebook() else None,\n",
    "        benchmark=True,\n",
    "        enable_checkpointing=False,\n",
    "    )\n",
    "    tuner = Tuner(trainer)\n",
    "    model = VitsLightning(reset_optimizer=reset_optimizer, **hparams)\n",
    "\n",
    "    # automatic batch size scaling\n",
    "    batch_size = hparams.train.batch_size\n",
    "    batch_split = str(batch_size).split(\"-\")\n",
    "    batch_size = batch_split[0]\n",
    "    init_val = 2 if len(batch_split) <= 1 else int(batch_split[1])\n",
    "    max_trials = 25 if len(batch_split) <= 2 else int(batch_split[2])\n",
    "    if batch_size == \"auto\":\n",
    "        batch_size = \"binsearch\"\n",
    "    if batch_size in [\"power\", \"binsearch\"]:\n",
    "        model.tuning = True\n",
    "        tuner.scale_batch_size(\n",
    "            model,\n",
    "            mode=batch_size,\n",
    "            datamodule=datamodule,\n",
    "            steps_per_trial=1,\n",
    "            init_val=init_val,\n",
    "            max_trials=max_trials,\n",
    "        )\n",
    "        model.tuning = False\n",
    "    else:\n",
    "        batch_size = int(batch_size)\n",
    "    # automatic learning rate scaling is not supported for multiple optimizers\n",
    "    \"\"\"if hparams.train.learning_rate  == \"auto\":\n",
    "    lr_finder = tuner.lr_find(model)\n",
    "    LOG.info(lr_finder.results)\n",
    "    fig = lr_finder.plot(suggest=True)\n",
    "    fig.savefig(model_path / \"lr_finder.png\")\"\"\"\n",
    "\n",
    "    trainer.fit(model, datamodule=datamodule)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # train(CONFIG_FILE, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir {MODEL_PATH}\n",
    "# !svc train --model-path {MODEL_PATH}\n",
    "train(CONFIG_FILE, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cluster model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Train cluster model (Optional)\n",
    "!svc train-cluster --output-path {MODEL_PATH}kmeans.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Get the author's voice as a source\n",
    "import random\n",
    "NAME = str(random.randint(1, 49))\n",
    "TYPE = \"fsd50k\" #@param [\"\", \"digit\", \"dog\", \"fsd50k\"]\n",
    "CUSTOM_FILEPATH = \"\" #@param {type: \"string\"}\n",
    "if CUSTOM_FILEPATH != \"\":\n",
    "    NAME = CUSTOM_FILEPATH\n",
    "else:\n",
    "    # it is extremely difficult to find a voice that can download from the internet directly\n",
    "    if TYPE == \"dog\":\n",
    "        !wget -N f\"https://huggingface.co/datasets/437aewuh/dog-dataset/resolve/main/dogs/dogs_{NAME:.0000}.wav\" -O {NAME}.wav\n",
    "    elif TYPE == \"digit\":\n",
    "        # george, jackson, lucas, nicolas, ...\n",
    "        !wget -N f\"https://github.com/Jakobovski/free-spoken-digit-dataset/raw/master/recordings/0_george_{NAME}.wav\" -O {NAME}.wav\n",
    "    elif TYPE == \"fsd50k\":\n",
    "        !wget -N f\"https://huggingface.co/datasets/Fhrozen/FSD50k/blob/main/clips/dev/{10000+int(NAME)}.wav\" -O {NAME}.wav\n",
    "    else:\n",
    "        !wget -N f\"https://zunko.jp/sozai/utau/voice_{\"kiritan\" if NAME < 25 else \"itako\"}{NAME % 5 + 1}.wav\" -O {NAME}.wav\n",
    "\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "display(Audio(f\"{NAME}.wav\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Use trained model\n",
    "#@markdown **Put your .wav file in `so-vits-svc-fork/audio` directory**\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "AUDIO_PATH = 'drive/MyDrive/so-vits-svc-fork/audio/' #@param {type: \"string\"}\n",
    "NAME = \"test\" #@param {type: \"string\"}\n",
    "TRANSPOSE = 0 #@param {type: \"number\"}\n",
    "F0_METHOD = \"crepe\" #@param [\"crepe\", \"crepe-tiny\", \"parselmouth\", \"dio\", \"harvest\"]\n",
    "\n",
    "AUTO_PREDICT_F0 = True #@param {type:\"boolean\"}\n",
    "if AUTO_PREDICT_F0:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -m {MODEL_PATH} -c {MODEL_PATH}config.json -t {TRANSPOSE} -fm {F0_METHOD}\n",
    "else:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -m {MODEL_PATH} -c {MODEL_PATH}config.json -t {TRANSPOSE} -fm {F0_METHOD} -na\n",
    "\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.wav\", autoplay=False))\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.out.wav\", autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##@title Use trained model (with cluster)\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "AUDIO_PATH = 'drive/MyDrive/so-vits-svc-fork/audio/' #@param {type: \"string\"}\n",
    "NAME = \"test\" #@param {type: \"string\"}\n",
    "SPEAKER = \"kiritan\" #@param {type: \"string\"}\n",
    "TRANSPOSE = 0 #@param {type: \"number\"}\n",
    "F0_METHOD = \"crepe\" #@param [\"crepe\", \"crepe-tiny\", \"parselmouth\", \"dio\", \"harvest\"]\n",
    "\n",
    "AUTO_PREDICT_F0 = True #@param {type:\"boolean\"}\n",
    "if AUTO_PREDICT_F0:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -s {SPEAKER} -r 0.1 -m {MODEL_PATH} -c {MODEL_PATH}config.json -k {MODEL_PATH}kmeans.pt -t {TRANSPOSE} -fm {F0_METHOD}\n",
    "else:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -s {SPEAKER} -r 0.1 -m {MODEL_PATH} -c {MODEL_PATH}config.json -k {MODEL_PATH}kmeans.pt -t {TRANSPOSE} -fm {F0_METHOD} -na\n",
    "\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.wav\", autoplay=False))\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.out.wav\", autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backup models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = CHARACTER\n",
    "\n",
    "!rm -f *.{{md5,sha1,sha256}}.sum\n",
    "!rclone mkdir colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG}\n",
    "!rclone copy colab:/so-vits-svc-fork/models/{DATASET_NAME}/ . --config {RCLONE_CONFIG} --no-traverse --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "# !rclone ls colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG} --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "# !rclone ls . --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "# !rclone ls drive/MyDrive/so-vits-svc-fork/logs/44k/ --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "!sort -k 2.3bn -k 2b -k 1 -u <(rclone hashsum MD5 drive/MyDrive/so-vits-svc-fork/logs/44k/ --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\") <(cat checksum.md5.sum) -o checksum.md5.sum\n",
    "!sort -k 2.3bn -k 2b -k 1 -u <(rclone hashsum SHA1 drive/MyDrive/so-vits-svc-fork/logs/44k/ --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\") <(cat checksum.sha1.sum) -o checksum.sha1.sum\n",
    "!sort -k 2.3bn -k 2b -k 1 -u <(rclone hashsum SHA256 drive/MyDrive/so-vits-svc-fork/logs/44k/ --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\") <(cat checksum.sha256.sum) -o checksum.sha256.sum\n",
    "\n",
    "!cp -f *.{{md5,sha1,sha256}}.sum drive/MyDrive/so-vits-svc-fork/logs/44k/\n",
    "!rclone copy drive/MyDrive/so-vits-svc-fork/logs/44k/ colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG} --no-traverse --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "!rclone copy . colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG} --no-traverse --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "\n",
    "!rclone check . drive/MyDrive/so-vits-svc-fork/logs/44k/ --one-way --download --differ - --missing-on-dst - --error - --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "!rclone check drive/MyDrive/so-vits-svc-fork/logs/44k/ colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG} --one-way --download --differ - --missing-on-dst - --error - --filter \"+ config.json\" --filter \"- {{D,G}}_0.pth\" --filter \"+ {{D,G}}_\\d\\d\\d*.pth\" --filter \"+ kmeans.pt\" --filter \"- *.{{md5,sha1,sha256}}.sum\" --filter \"- **\"\n",
    "!rclone check . colab:/so-vits-svc-fork/models/{DATASET_NAME}/ --config {RCLONE_CONFIG} --one-way --download --differ - --missing-on-dst - --error - --filter \"+ /*.{{md5,sha1,sha256}}.sum\" --filter \"- **\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/tree/main\n",
    "!wget -N \"https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/resolve/main/riri/G_riri_220.pth\"\n",
    "!wget -N \"https://huggingface.co/TachibanaKimika/so-vits-svc-4.0-models/resolve/main/riri/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "AUDIO_PATH = 'drive/MyDrive/so-vits-svc-fork/audio/' #@param {type: \"string\"}\n",
    "NAME = \"test\" #@param {type: \"string\"}\n",
    "\n",
    "AUTO_PREDICT_F0 = True #@param {type:\"boolean\"}\n",
    "if AUTO_PREDICT_F0:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -c config.json -m G_riri_220.pth\n",
    "else:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav -c config.json -m G_riri_220.pth -na\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.wav\", autoplay=False))\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.out.wav\", autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title https://huggingface.co/therealvul/so-vits-svc-4.0/tree/main\n",
    "!wget -N \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Pinkie%20(speaking%20sep)/G_166400.pth\"\n",
    "!wget -N \"https://huggingface.co/therealvul/so-vits-svc-4.0/resolve/main/Pinkie%20(speaking%20sep)/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "AUDIO_PATH = 'drive/MyDrive/so-vits-svc-fork/audio/' #@param {type: \"string\"}\n",
    "NAME = \"test\" #@param {type: \"string\"}\n",
    "\n",
    "AUTO_PREDICT_F0 = True #@param {type:\"boolean\"}\n",
    "if AUTO_PREDICT_F0:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav --speaker \"Pinkie {neutral}\" -c config.json -m G_166400.pth\n",
    "else:\n",
    "    !svc infer {AUDIO_PATH}{NAME}.wav --speaker \"Pinkie {neutral}\" -c config.json -m G_166400.pth -na\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.wav\", autoplay=False))\n",
    "display(Audio(f\"{AUDIO_PATH}{NAME}.out.wav\", autoplay=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit ('3.8.9')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "1505505db2c7996cb37b93326518de8bfa05b3af11708fd695a6c80c3daf3624"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
